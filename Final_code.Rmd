---
title: "Code"
author: "Sandeep"
date: "11/21/2019"
output:
  word_document: default
  html_document: default
---

I install the following libraries as below.
```{r include=FALSE,echo=FALSE}


#install.packages("pacman")
library(pacman)
pacman::p_load(randomForest,tidyverse,dplyr,caret,scales,dataPreparation,e1071,gbm,glmnet,corrplot,ranger,cluster,factoextra,animation,DAAG,ggplot2gridExtra)
```
###EDA

```{r, warning=FALSE,echo=FALSE}
games<-read.csv("Video_Games_Sales_as_at_22_Dec_2016.csv")
View(games)
sapply(games,class)
str(games)
games$User_Count<-as.numeric(as.character(games$User_Count))
games$User_Score<-as.numeric(as.character(games$User_Score))

games$Critic_Score[is.na(games$Critic_Score)]<-mean(games$Critic_Score,na.rm = TRUE)
games$Critic_Count[is.na(games$Critic_Count)]<-mean(games$Critic_Count,na.rm = TRUE)
games$User_Count[is.na(games$User_Count)]<-mean(games$User_Count,na.rm = TRUE)
games$User_Score[is.na(games$User_Score)]<-mean(games$User_Score,na.rm = TRUE)

sum(is.na(games))
```
  First, I analyze the correlation plot. We can see that the Global sales is more correlated with North America Sales, Europe sales, Japan sales and other sales as expected. Also we can see a correlation between user and critic score, critic score and critic count. User count and user score do not have much of a correlation. 
```{r, warning=FALSE,echo=FALSE}
num_cols<-select_if(games, is.numeric)

cor<-cor(num_cols)
corrplot(cor, method = "number") # Display the correlation coefficient
corrplot(cor,type="upper",method="number")
```

 I use log of Sales for showing distribution. All the genres have a wide distribution. Sports being the most popular genre has the widest distribution

```{r, warning=FALSE,echo=FALSE}
theme_g<-theme(axis.text.x = element_text(angle=90),plot.title  =element_text(size=8),panel.background = element_rect(fill="black"),
                         panel.grid.major = element_blank(),
                         panel.grid.minor=element_blank())
g1<-ggplot(games,aes(x=log(Global_Sales),fill=Genre))+geom_density(alpha=0.3)+labs(x="Global Sales")+theme_g
g2<-games %>% select(Rating)%>%count(Rating)%>%ggplot(aes(x=Rating,y=n,fill=Rating))+geom_bar(stat="identity")+theme_g
g1


```
Several Ratings like M, AO, E etc exists for video games, what does it actually mean. Rating refers to age appropriateness for the games.
. E - Everyone
. E10+- Everyone 10+
. T - Teen
. M - Mature
. EC- Early Childhood
. AO- Adults Only
. RP- Rating Pending
. K A - Kids to Adults###Rating and Sales
AO, EC, K-A, RP do not have much of data . E, E10+,T ,M . In E10+ category, Sports has the highest median and the highest sales. Puzzle, Racing, RPG, Action are all popular for E10+. RPG, Shooter and Action are best selling among Mature genre. For teens, all genres expect adventure, puzzle and strategy seem to do well .
We conclude that genres and rating have an impact on global sales.
```{r, warning=FALSE,echo=FALSE}
ggplot(games,aes(x=Genre,y=log(Global_Sales),col=Rating))+geom_boxplot(varwidth = TRUE)+facet_wrap(~Rating)+
  theme(axis.text.x=element_text(angle=90))
```
```{r, warning=FALSE,echo=FALSE}
options(repr.plot.width=5, repr.plot.height=4)
ggplot(games,aes(x=Critic_Score,y=User_Score)) + stat_binhex() + scale_fill_gradientn(colours=c("black","yellow"),name = "Frequency",na.value=NA)+theme(panel.background = element_rect(fill="black"),
                         panel.grid.major = element_blank(),
                         panel.grid.minor=element_blank())+geom_smooth(method="lm",col="yellow4")
```
We can see that between 1980-1995 , the sales was too low because it was still in development stages and not popular. The sales reached started to pick up after 1995 and reached the peak during 2005-
2011. After that it started to drop again. Action games seem to perform the best among all genres followed by Sports.
```{r, warning=FALSE,echo=FALSE}
games %>% select(Name,Genre,Year_of_Release)%>% filter(!Genre=='')%>% group_by(Year_of_Release,Genre)%>% summarise(no_of_games=n())%>%ggplot(aes(x=Year_of_Release,y=no_of_games,group=Genre,col=Genre))+geom_point(size=0.5)+geom_line()+theme(legend.position = "bottom",axis.text.x = element_text(angle=90))+xlab("Year of release")+ylab("No. of games")
```
```{r, warning=FALSE,echo=FALSE}

games%>%select(Genre,User_Score)%>%filter(!Genre=='')%>%ggplot(aes(x=Genre,y=User_Score,col=Genre))+geom_jitter(size=0.3)+theme(legend.position = "bottom",axis.text.x = element_text(angle=90))

sum(is.na(games$Genre))
```
Nintendo, Electonic Arts, Activision are the top 3 performers globally. People certainly have a preference for games by these publishers.
```{r, warning=FALSE,echo=FALSE}
games%>%select(Publisher,Global_Sales)%>%group_by(Publisher)%>%summarise(Totsales=sum(Global_Sales))%>%arrange(desc(Totsales))%>%head(10)%>%
                                                                           ggplot(aes(x=factor(Publisher,level=Publisher),y=Totsales,fill=Publisher))+geom_col()+theme(legend.position = "none",axis.text.x = element_text(angle=90))+scale_fill_brewer(palette="Spectral")+labs(x="Publisher",y="Total Sales",title="Sales by Publishers -Top 10")
```
Nintendo, EA and Activison do very well in North America and Europe. but in Japan Nintendo is followed by Namco and Konami indication local dominance over global powers. In other countries EA occupies the first place followed by Nintendo and Sony
```{r, warning=FALSE,echo=FALSE}
g1<-games%>%select(Publisher,NA_Sales)%>%group_by(Publisher)%>%summarise(Totsales=sum(NA_Sales))%>%arrange(desc(Totsales))%>%head(5)%>%
                                                                           ggplot(aes(x=factor(Publisher,level=Publisher),y=Totsales,fill=Publisher))+geom_col()+theme(legend.position = "none",axis.text.x = element_text(angle=90))+scale_fill_brewer(palette="Spectral")+labs(x="Publisher",y="Total Sales",title="Sales by Publishers -Top 10")

g2<-games%>%select(Publisher,EU_Sales)%>%group_by(Publisher)%>%summarise(Totsales=sum(EU_Sales))%>%arrange(desc(Totsales))%>%head(5)%>%
                                                                           ggplot(aes(x=factor(Publisher,level=Publisher),y=Totsales,fill=Publisher))+geom_col()+theme(legend.position = "none",axis.text.x = element_text(angle=90))+scale_fill_brewer(palette="Spectral")+labs(x="Publisher",y="Total Sales",title="Sales by Publishers -Top 10")

g3<-games%>%select(Publisher,JP_Sales)%>%group_by(Publisher)%>%summarise(Totsales=sum(JP_Sales))%>%arrange(desc(Totsales))%>%head(5)%>%
                                                                           ggplot(aes(x=factor(Publisher,level=Publisher),y=Totsales,fill=Publisher))+geom_col()+theme(legend.position = "none",axis.text.x = element_text(angle=90))+scale_fill_brewer(palette="Spectral")+labs(x="Publisher",y="Total Sales",title="Sales by Publishers -Top 10")

g4<-games%>%select(Publisher,Other_Sales)%>%group_by(Publisher)%>%summarise(Totsales=sum(Other_Sales))%>%arrange(desc(Totsales))%>%head(5)%>%
ggplot(aes(x=factor(Publisher,level=Publisher),y=Totsales,fill=Publisher))+geom_col()+theme(legend.position = "none",axis.text.x = element_text(angle=90))+scale_fill_brewer(palette="Spectral")+labs(x="Publisher",y="Total Sales",title="Sales by Publishers -Top 10")
g1
g2
g3
g4

```
All the publishers have a peak point at 2008 and then started to plummet. Why? Because that is when mobile phones began to become popular and hence people found an alternative to kill time. Some of the games became available in mobile apps.
```{r, warning=FALSE,echo=FALSE}
games%>%select(Publisher,Global_Sales,Year_of_Release)%>%group_by(Year_of_Release,Publisher)%>%summarise(Totsales=sum(Global_Sales)) %>%                                       ggplot(aes(x=Year_of_Release,y=Totsales,group=Publisher,fill=Publisher))+geom_area()+theme(legend.position = "none",axis.text.x = element_text(angle=90))+labs(x="Publisher",y="Total Sales",title="Sales by Publishers -Top 10")
```
Next we analyze the best selling video games. Wii Sports is the best seller followed by GTA 5 and Super Mario
```{r, warning=FALSE,echo=FALSE}
games %>%select(Name,Global_Sales)%>%group_by(Name)%>%summarise(global_sales=sum(Global_Sales))%>%arrange(desc(global_sales))%>%head(10)%>%ggplot(aes(x=factor(x = Name,level=Name),y=global_sales,group=1,fill=Name))+geom_col()+scale_color_gradientn(colours = heat.colors(20))+theme(legend.position = "none",axis.text.x = element_text(angle=90))+labs(xtitle="Top Video games by Sales")
```
###Predictive modelling
I am going to predict Video game Sales and what factors it depends on. For this i created 5 models:
1. Linear regression
2. Random Forest
3. Gradient Boosting
4. Ridge Regression
5. KNN
For all the above models, i have used RMSE as the parameter for comparison. I have used cross validation to fine tune the hyperparameters.


Function to calculate mode
```{r, warning=FALSE,echo=FALSE }
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

```
Here i found lot of missing values. The critic score particularly found only in half of the records.I replaced the categorical columns by mode of the column . I created a new column called newPlatform that aggregates as PlayStation,XBOX,Nintendo,Sega and the rest as Others. There are too many different platforms and most of them represent a very small percent of games. I am going to group platforms to reduce the number of features.

```{r , warning=FALSE,echo=FALSE}
games<-read.csv("Video_Games_Sales_as_at_22_Dec_2016.csv")
games$User_Count<-as.numeric(as.character(games$User_Count))
games$User_Score<-as.numeric(as.character(games$User_Score))
games$Critic_Count<-as.numeric(as.character(games$Critic_Count))
games$Critic_Score<-as.numeric(as.character(games$Critic_Score))
games$Year_of_Release = as.numeric(as.character(games$Year_of_Release))

#Create another games variable by filtering rows without critic score
games2<-filter(games,!is.na(games$Critic_Score))

games2$User_Score[is.na(games2$User_Score)]<-median(games2$User_Score,na.rm = TRUE)
games2$User_Count[is.na(games2$User_Count)]<-median(games2$User_Count,na.rm = TRUE)
games2<-na.omit(games2)
games$Publisher<-as.character(games$Publisher)
games$Publisher[games$Publisher=="N/A"]<-getmode(games$Publisher)
games$Publisher<-as.factor(games$Publisher)
games$Name<-as.character(games$Name)
games$Rating<-as.character(games$Rating)
games$Rating[games$Rating==""]<-getmode(games$Rating)
games$Rating<-as.factor(games$Rating)
games$Developer<-as.character(games$Developer)
games$Developer[games$Developer==""]<-getmode(games$Developer)
games$Developer<-as.factor(games$Developer)
games$age<- 2019-games$Year_of_Release


sony<-c('PS','PS2','PS3','PS4' ,'PSP','PSV')
microsoft<-c('X360','XB','XOne')
pc<-c('PC')
nintendo<-c('3DS','DS','GBA','GC','N64','Wii','WiiU')
sega<-c('DC')


newPlatform<-function(x){
    if (x %in% sony == TRUE) {return('PS')}
    else if(x %in% microsoft == TRUE) {return('XBOX')}
    else if(x %in% pc ==TRUE){return('PC')}
    else if(x %in% nintendo == TRUE) {return('NINTENDO')}
    else if(x %in% sega == TRUE) {return('SEGA')}
    else{return('OTHER')}
}
games$newPlatform<-sapply(games$Platform,newPlatform)
#VIsualizing the newly created variables new Platform
games$newPlatform<-as.factor(games$newPlatform)
count<-games%>%group_by(newPlatform)%>%summarise(count=n())
count$fraction = count$count / sum(count$count)

# Compute the cumulative percentages (top of each rectangle)
count$ymax = cumsum(count$fraction)

# Compute the bottom of each rectangle
count$ymin = c(0, head(count$ymax, n=-1))
count$label <- paste0(count$newPlatform, "\n value: ", count$count)
count$labelPosition <- (count$ymax + count$ymin) / 2

# Make the plot
ggplot(count, aes(ymax=ymax, ymin=ymin, xmax=4, xmin=3, fill=newPlatform)) +
     geom_rect() +
     coord_polar(theta="y") + # Try to remove that to understand how the chart is built initially
     xlim(c(2, 4)) +
     geom_label( x=3.5, aes(y=labelPosition, label=label), size=2) +
    theme_void()+  theme(legend.position = "none")

```

###Weighted Score and Developer Rating
In order to handle for the missing values in critic and user rating, i created 2 new features: weighted score and developer rating. THe weighted score is calculated as the average of user and critic rating.for Develoepr rating, I find percent of all games created by each developer, then calculate cumulative percent starting with developers with the least number of games. Ultimately, I divide them into 5 groups each containing 20%.Higher the rating, the more number of games it developed

```{r, warning=FALSE,echo=FALSE}

devs<-games2%>%group_by(Developer)%>%summarise(count=n())
games2$weighted_score<-((games2$Critic_Score*games2$Critic_Count) +(games2$User_Score*10*games2$User_Count))/(games2$User_Count+games2$Critic_Count)
sapply(games2,class)
games2$Developer<-as.character(games2$Developer)
m_score<-games2%>%group_by(Developer)%>%summarise(count=n(),
                                                  mscore=mean(weighted_score,na.rm=TRUE))%>%arrange(count)
m_score$frac<- m_score$count/sum(m_score$count)
m_score$top_percent<-cumsum(m_score$frac)*100
n_groups=5
m_score$group<-floor((m_score$top_percent*n_groups)/100)+1
m_score[1038,"group"]<-5
m_score<-filter(m_score,!m_score$Developer=="")
```


```{r, warning=FALSE,echo=FALSE}
merged_games<-  merge(games,m_score,by="Developer",all = TRUE)

merged_games$User_Score[is.na(merged_games$User_Score)]<-0
merged_games$User_Count[is.na(merged_games$User_Count)]<-0
merged_games$Critic_Score[is.na(merged_games$Critic_Score)]<-0

merged_games$Critic_Count[is.na(merged_games$Critic_Count)]<-0
merged_games<-subset(merged_games,select = -c(mscore,frac,top_percent,count))

merged_games$weighted_score<-((merged_games$Critic_Score*merged_games$Critic_Count) +(merged_games$User_Score*10*merged_games$User_Count))/(merged_games$User_Count+merged_games$Critic_Count)
colnames(merged_games)[colnames(merged_games)=="group"]<-"Developer_Rating"
merged_games$Developer_Rating[is.na(merged_games$Developer_Rating)]<-0
merged_games$weighted_score[is.na(merged_games$weighted_score)]<-0
merged_games$Rating<-as.character(merged_games$Rating)
merged_games$Rating[merged_games$Rating==""]<-getmode(merged_games$Rating[!merged_games$Rating==""])
merged_games$Rating<-as.factor(merged_games$Rating)
merged_games$newPlatform<-as.factor(merged_games$newPlatform)
merged_games$Developer<-as.character(merged_games$Developer)
merged_games$Developer[merged_games$Developer==""]<-"Not Available"
merged_games$Developer<-as.factor(merged_games$Developer)

merged_games$is_score<-ifelse(((merged_games$User_Score==0)&(merged_games$User_Score==0)),0,1)
merged_games$rownum <- seq.int(nrow(merged_games))
region<-subset(merged_games,select = c(NA_Sales,EU_Sales,JP_Sales,Other_Sales))
index<-max.col(region,"first")
in1<-region[cbind(1:nrow(region),index)]
nam<-names(region)[index]
df<-data.frame(in1,nam)
df$rownum <- seq.int(nrow(df))
merged_games<-merge(merged_games,df,on="rownum")
merged_games<-subset(merged_games,select=-c(in1))
#I created a column Country which holds theregion that got the  highest of the region sales
country<-function(x){
if(x=="NA_Sales"){
  return ("North America")
}else if(x=="JP_Sales"){
  return( "Japan")
}else if(x=="EU_Sales"){
  return("Europe")
}else{
  return("Other")
}
}
merged_games$country<-sapply(merged_games$nam,country)
merged_games$country<-as.factor(merged_games$country)
```

Once all the above feature transforamtion and data cleaning was done, it was time to build models. We split into 70% training data and 30% test data. I then scaled the data to reduce variance.

```{r, warning=FALSE,echo=FALSE}
set.seed(123)
cols<-c("Genre","Global_Sales","Critic_Score","Critic_Count","User_Score","User_Count","Developer","age","Rating","newPlatform","Developer_Rating","weighted_score","country")
model_data<-merged_games[cols]
model_data<-na.omit(model_data)

model_data$logsales<-log(model_data$Global_Sales)

trainIndex <- createDataPartition(model_data$Global_Sales, p = .7, 
                                  list = FALSE, 
                                  times = 1)

games_train <- model_data[ trainIndex,]
games_test  <- model_data[-trainIndex,]

#varnames<-c("Platform","Genre","Publisher","Global_Sales","Year_of_Release","Rating","newPlatform","Developer")

scales <- build_scales(dataSet = games_train, cols = c("Critic_Score", "Critic_Count","User_Score","User_Count","age","Developer_Rating","weighted_score"), verbose = TRUE)
games_train <- fastScale(dataSet = games_train, scales = scales, verbose = TRUE)
games_test <- fastScale(dataSet = games_test, scales = scales, verbose = TRUE)
games_test<-filter(games_test,!games_test$Rating=="AO")

rmse <- function(error)
{
  sqrt(mean(error^2))
}

```
###Linear Regression
I have iniitally run a linear regression model.
```{r, warning=FALSE,echo=FALSE}
set.seed(123)
linear_mod<-lm(logsales~newPlatform+Genre+Critic_Score+Critic_Count+User_Count+User_Score+age+Developer_Rating+weighted_score+country,games_train)

summary(linear_mod)
```
newPlatform,Critic Score, Critic Count.Uer Score,User Count,age ,DeveloperRating,weighted score and country are all significant variables in this model. Genre seems highly insignifcant and does not contribute to prediction of the sales. The R-suqare is 33.4%.

```{r, warning=FALSE,echo=FALSE}
train_pred<-exp(predict(linear_mod, games_train))
error1<-games_train$Global_Sales-train_pred
rmse(error1[!is.na(error1)])
```
I wanted to use cross valdiation to improve the estimates. The model might be overfitting and hence i run a 3 fold cross validation
```{r, warning=FALSE,echo=FALSE}
train_control <- trainControl(method="cv", number=10)
model <- train(logsales~newPlatform+Genre+Critic_Score+Critic_Count+User_Count+User_Score+age+Rating+Developer_Rating+weighted_score+country, data=games_train, trControl=train_control, method="lm",na.action = na.exclude)
model
#CV PLot
#cvlm<-cv.lm(data=games_train,linear_mod,m=3)


predictions <- exp(predict(model, games_test))
plot(games_test$Global_Sales,predictions)

error<-games_test$Global_Sales-predictions
#Test error
```
The resulting test error was 2.1. Lets see if we can reduce the error value in the next models.
```{r, warning=FALSE,echo=FALSE}
rmse(error[!is.na(error)])
```

###Random Forest
  I ran Random forest model which is an ensemble method over decision trees. THey are popular for reducing overfitting and providing good estimates.
  I initally experiment wiht the number of variables at each split from 1 to 10. Both out of bag error and test errors have been calculated.

```{r, warning=FALSE,echo=FALSE}
oob.err=double(10)
test.err=double(10)
#mtry is no of Variables randomly chosen at each split
# for(mtry in 1:5) 
# {
#   rf=randomForest(logsales~newPlatform+Genre+Critic_Score+Critic_Count+User_Count+User_Score+age+Rating+Developer_Rating+weighted_score+country , data = games_train,mtry=mtry,num.trees =400,na.action = na.exclude) 
#   oob.err[mtry] = (rf$mse[400]) #Error of all Trees fitted
#   
#   pred<-exp(predict(rf,model_data[-trainIndex,])) #Predictions on Test Set for each Tree
#  # test.err[mtry]=  with(model_data[-trainIndex,],mean(!is.na((Global_Sales - pred)^2)))
#  #Mean Squared Test Error
#   error<-games_test$Global_Sales-pred
#   test.err[mtry]<-mean((error[!is.na(error)])^2)
#   
#   cat(mtry," ") #printing the output to the console
#   
# }
# 
# for(mtry in 6:10) 
# {
#   rf=randomForest(logsales~newPlatform+Genre+Critic_Score+Critic_Count+User_Count+User_Score+age+Rating+Developer_Rating+weighted_score+country , data = games_train,mtry=mtry,num.trees =400,na.action = na.exclude) 
#   oob.err[mtry] = (rf$mse[400]) #Error of all Trees fitted
#   
#   pred<-exp(predict(rf,model_data[-trainIndex,])) #Predictions on Test Set for each Tree
#  # test.err[mtry]=  with(model_data[-trainIndex,],mean(!is.na((Global_Sales - pred)^2)))
#  #Mean Squared Test Error
#   error<-games_test$Global_Sales-pred
#   test.err[mtry]<-mean((error[!is.na(error)])^2)
#   
#   cat(mtry," ") #printing the output to the console
#   
# }
# 
# 
# 
# matplot(1:10, cbind(sqrt(oob.err[1:10]),sqrt(test.err[1:10])), pch=19 , col=c("red","blue"),type="b",ylab="Root Mean Squared Error",xlab="Number of Predictors Considered at each Split")
# legend("topright",legend=c("Out of Bag Error","Test Error"),pch=19, col=c("red","blue"))
```
We choose 3 as the numebr of predictors to be considered at the split as the curve seems to overfit after 3 . I rebuilt the model and found out the test and train error. 
Train error- 1.045993

Test error- 1.095678


```{r, warning=FALSE,echo=FALSE}
rfmodel<-randomForest::randomForest(
logsales~newPlatform+Genre+Critic_Score+Critic_Count+User_Count+User_Score+age+Rating+Developer_Rating+weighted_score+country,  data = games_train,mtry=3,na.action=na.exclude,num.trees=200)
plot(rfmodel)
#R-square
mean(rfmodel$rsq)
#Train RMSE

train_error<-sqrt(mean(rfmodel$mse))
train_error
p<-exp(predict(rfmodel,games_test))
error<-games_test$Global_Sales-p
test_error<-sqrt(mean((error[!is.na(error)])^2))
test_error
```
The importance fucntion gives the important variables in the order of importance.All of them seem to have a non zero effect on Sales.
```{r,include=FALSE}
randomForest::importance(rfmodel,type=1)
randomForest::varImpPlot(rfmodel,type = 2)
```

###Gradient Boosting
For Gradient Boosting, I experiemnt with learning rates 0.01,0.10.3 and depth of the tree 1,3,5
```{r,warning=FALSE,echo=FALSE}
hyper_grid <- expand.grid(
  shrinkage = c(.01, .1, .3),
  interaction.depth = c(1, 3, 5),
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)
for (i in 1:nrow(hyper_grid)){
set.seed(123)
gradient_boost<-gbm(logsales~newPlatform+Genre+Critic_Score+Critic_Count+User_Count+User_Score+age+Rating+Developer_Rating+weighted_score+country,  data = games_train,interaction.depth = hyper_grid$interaction.depth[i],shrinkage = hyper_grid$shrinkage[i],distribution = "gaussian",n.trees = 1000)
  hyper_grid$optimal_trees[i] <- which.min(gradient_boost$train.error)
  hyper_grid$min_RMSE[i] <- sqrt(min(gradient_boost$train.error))

}
```
Learning rate=0.3 and Depth=5 gave the lowest errors.
```{r}
hyper_grid<-hyper_grid %>% 
  dplyr::arrange(min_RMSE) %>%
  head(10)
```
The optimum values are now used to train the model. I find out from the plot that for all number of trees the test error is quite high around 2 while the train error drops below 1. This is a case of good generalization of the train set but not unknown data. This is a clear case of overfitting. Hence I reject gradient boosting model for my prediction.
```{r}
tuned_gbm<-gbm(logsales~newPlatform+Genre+Critic_Score+Critic_Count+User_Count+User_Score+age+Rating+Developer_Rating+weighted_score+country,  data = games_train,interaction.depth = 5,shrinkage = 0.3,distribution = "gaussian",n.trees = 1000)
trees<-seq(from=100,to=1000,by=100)

train_errors<-sqrt(tuned_gbm$train.error)
train_errors1<-train_errors[trees]


predmat<-predict(tuned_gbm,games_test,n.trees = trees)
test_errors=double(10)
for (i in 1:10)
{
  error=games_test-predmat[,i]
  test_errors[i]=rmse(error[!is.na(error)])
  
}
matplot(trees,cbind(train_errors1,test_errors),pch=19 ,type="b",col=c("blue","green"),ylab="Root Mean Squared Error",xlab="Number of Trees")
legend("topright",legend=c("Train Error","Test Error"),pch=19, col=c("blue","green"))
```

###Ridge Regression
Inintally we want to tune the model by experimenting with lambda values.Then I find the best value and rebuild the model and test it with the test data. THe optimum value of lambda was 0.01. I obtained a test error of 2.053557 which was lesser than linear regression but is way higher than random forest.
```{r, warning=FALSE,echo=FALSE}
lambda_seq <- 10^seq(-2,2, by = .1)
x<-model.matrix(logsales~newPlatform+Genre+Critic_Score+Critic_Count+User_Count+User_Score+age+Rating+Developer_Rating+weighted_score+country,games_train)

ridge_fit <- glmnet(x,games_train$logsales, alpha = 0, lambda  = lambda_seq)
summary(ridge_fit)

cv_fit <- cv.glmnet(x, games_train$logsales, alpha = 0, lambda = lambda_seq)
plot(cv_fit)

opt_lambda <- cv_fit$lambda.min
best_ridge<-cv_fit$glmnet.fit

#Rebuilding with best lambda value
fit <- glmnet(x,games_train$logsales, alpha = 0, lambda  = 0.01)

summary(fit)
coef(fit)
xtest<-model.matrix(logsales~newPlatform+Genre+Critic_Score+Critic_Count+User_Count+User_Score+age+Rating+Developer_Rating+weighted_score+country,games_test)
y_predicted <- exp(predict(ridge_fit, s = opt_lambda, newx = xtest))
y<-games_test$Global_Sales

y_predicted1 <- exp(predict(ridge_fit, s = opt_lambda, newx = x))
y1<-games_train$Global_Sales

#Train Error
tr_error<-  rmse(y1- y_predicted1)
#Test error
test_error<-  rmse(y- y_predicted)
test_error

```
###KNN
K Nearest Neighbours is another model which requires no training before making predictions.New data can be added seamlessly which will not impact the accuracy of the algorithm. I needed to choose  a good value of K for prediction. Hence i plotted the train and test errors for different K values from 2 to 10. WE can see that after k=2, the training error becomes higher than the test error whihc is not ideal. Hence i choose K=2.
```{r , warning=FALSE,echo=FALSE}
train_error1<-double(10)
test_error1<-double(10)
View(train_error1)
for (i in 2:10)
{
  knn<-caret::knnreg(logsales~newPlatform+Genre+Critic_Score+Critic_Count+User_Count+User_Score+age+Rating+Developer_Rating+weighted_score+country,games_train,na.action=na.omit,k=i)
  pred<-exp(predict(knn, games_train))
  train_error<-games_train$Global_Sales-pred
  train_error1[i]<-mean(train_error^2)
  pred1<-exp(predict(knn, games_test))
  test_error<-games_test$Global_Sales-pred1
  test_error1[i]<-mean(test_error^2)
}
train_error1<-sqrt(train_error1)
test_error1<-sqrt(test_error1)
matplot(1:10,cbind(train_error1,test_error1),col=c("blue","green"),type="b",pch=19,ylab="Root Mean Squared Error",xlab="K value")
legend("topright",legend=c("Train Error","Test Error"),pch=19, col=c("blue","green"))

# We can see that after k=2, the test error becomes lower than the train error which is not ideal. Hence i chose k=2 as the optimum k value.
```
I rebuild the model using K=2. I achieved train error of 1.02 and test error of 1.17 which is quite good. 
```{r, warning=FALSE,echo=FALSE}
knn<-caret::knnreg(logsales~newPlatform+Genre+Critic_Score+Critic_Count+User_Count+User_Score+age+Rating+Developer_Rating+weighted_score+country,games_train,na.action=na.omit,k=2)
pred<-exp(predict(knn, games_train))
train_error<-games_train$Global_Sales-pred
train_error<-sqrt(mean(train_error^2))
pred1<-exp(predict(knn, games_test))
test_error<-games_test$Global_Sales-pred1
test_error<-sqrt(mean(test_error^2))
train_error
test_error

```
```{r, warning=FALSE,echo=FALSE}
errs<-c(2.095374,2.053557,1.173264,1.094527)
models<-c("Linear","Ridge","KNN", "Random Forest")
models<-as.factor(models)
mode(models)
barplot(errs,names.arg = models,col="blue",border="red",xlab="Models",ylab="Error(RMSE)",main="Model Comparison")
```



